{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Library Import"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove the hash below to install optuna\n",
    "# python3 -m pip install optuna\n",
    "\n",
    "# load external modules\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy import random\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.metrics import mean_squared_error\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preparation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_list = [[np.random.rand(), np.random.rand(), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, np.random.rand()-0.5, np.random.rand()-0.5]] # first row\n",
    "for i in range(1, 10000):\n",
    "    new_row = [0.8 * data_list[i-1][0] + data_list[i-1][1] + data_list[i-1][12],\n",
    "               -0.8 * data_list[i-1][1] + data_list[i-1][13],\n",
    "               0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "               data_list[i-1][12] + np.random.rand() - 0.5,\n",
    "               data_list[i-1][13] + np.random.rand() - 0.5]\n",
    "    data_list.append(new_row)\n",
    "d_o = pd.DataFrame(data_list) # to dataframe\n",
    "\n",
    "for i in range(0, len(d_o)):\n",
    "    d_o.loc[i, 2] = d_o.loc[i, 0] + 0.5 * d_o.loc[i, 1]\n",
    "    d_o.loc[i, 3] = 0.3 * d_o.loc[i, 0] + d_o.loc[i, 1] + 0.4 * d_o.loc[i, 2] * d_o.loc[i, 2]\n",
    "    d_o.loc[i, 4] = 0.5 * d_o.loc[i, 0] * d_o.loc[i, 1] - d_o.loc[i, 2] - 0.2 * d_o.loc[i, 3] * d_o.loc[i, 3]\n",
    "    d_o.loc[i, 5] = d_o.loc[i, 1] * d_o.loc[i, 4] - 0.6 * math.tanh(d_o.loc[i, 0] * d_o.loc[i, 3]) + d_o.loc[i, 2]\n",
    "    d_o.loc[i, 6] = 0.2 * d_o.loc[i, 0] * d_o.loc[i, 5] + math.tanh(d_o.loc[i, 1] + d_o.loc[i, 2]) + 0.4 * math.tanh(d_o.loc[i, 3] * d_o.loc[i, 4])\n",
    "    d_o.loc[i, 7] = 0.35 * d_o.loc[i, 6] * d_o.loc[i, 4] - 0.4 * d_o.loc[i, 0] * d_o.loc[i, 5] + math.tanh(d_o.loc[i, 1] + d_o.loc[i, 2]) - 0.8 * d_o.loc[i, 1] * d_o.loc[i, 1] + 0.5 * d_o.loc[i, 3]\n",
    "    d_o.loc[i, 8] = d_o.loc[i, 0] - 0.1 * d_o.loc[i, 1] * d_o.loc[i, 1] + 0.3 * d_o.loc[i, 2] * d_o.loc[i, 2] + d_o.loc[i, 3] * math.tanh(d_o.loc[i, 4] - d_o.loc[i, 5]) + math.tanh(d_o.loc[i, 6] * d_o.loc[i, 7])\n",
    "    d_o.loc[i, 9] = d_o.loc[i, 0]+d_o.loc[i, 3] * d_o.loc[i, 6] + d_o.loc[i, 5] * math.tanh(d_o.loc[i, 4] - d_o.loc[i, 7]) - d_o.loc[i, 2] * math.tanh(d_o.loc[i, 1] + d_o.loc[i, 8])\n",
    "    d_o.loc[i, 10] = 0.3 * d_o.loc[i, 5] + math.tanh(d_o.loc[i, 0] + d_o.loc[i, 8] * d_o.loc[i, 8]) - d_o.loc[i, 6] * math.tanh( d_o.loc[i, 1] -  d_o.loc[i, 9] *  d_o.loc[i, 9]) + 0.4 * ( d_o.loc[i, 2] *  d_o.loc[i, 2] +  d_o.loc[i, 3] +  d_o.loc[i, 4]) +  d_o.loc[i, 7]\n",
    "    d_o.loc[i, 11] =0.45 * d_o.loc[i, 0] * d_o.loc[i, 2] - 0.6 * d_o.loc[i, 4] * d_o.loc[i, 1] + 0.1 * d_o.loc[i, 3] * d_o.loc[i, 3] * d_o.loc[i, 5] + math.tanh(d_o.loc[i, 6] / d_o.loc[i, 8] + d_o.loc[i, 7] * d_o.loc[i, 9]) + 0.5 * d_o.loc[i, 10]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_to_scale = d_o.iloc[:, 1:]\n",
    "data_standardized = scaler.fit_transform(data_to_scale)\n",
    "columns_to_keep = d_o.columns[0]\n",
    "data = pd.DataFrame(data_standardized, columns=data_to_scale.columns)\n",
    "data.insert(loc=0, column=columns_to_keep, value=d_o[columns_to_keep])\n",
    "\n",
    "x = data.iloc[:, 2:12]\n",
    "u = data.iloc[:, 12:14]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rc('font', family='Times New Roman')\n",
    "# Plot columns in u and label them as u1 and u2\n",
    "plt.figure(figsize=(8, 4), dpi=300)\n",
    "for i in range(2):  # Assuming you want to plot columns 12 and 13\n",
    "    plt.plot(u.iloc[:, i], label=f'u{i + 1}')\n",
    "plt.title('u Columns')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.savefig('u_output.jpg', format='jpeg', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Plot columns in x and label them as P3 to P12\n",
    "plt.figure(figsize=(8, 4), dpi=300)\n",
    "for i in range(10):  # Assuming you want to plot columns 2 to 11\n",
    "    plt.plot(x.iloc[:, i], label=f'P{i + 3}')\n",
    "plt.title('P Column')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.savefig('P_output.jpg', format='jpeg', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train = x.iloc[:7999]\n",
    "u_train = u.iloc[:7999]\n",
    "P5_train = x.iloc[1:8000, 2]\n",
    "P5_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "x_test = x.iloc[7999:9999]\n",
    "x_test.reset_index(drop=True, inplace=True)\n",
    "u_test = u.iloc[7999:9999]\n",
    "u_test.reset_index(drop=True, inplace=True)\n",
    "P5_test = x.iloc[8000:, 2]\n",
    "P5_test.reset_index(drop=True, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# VAE Model Build"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def encoder(x, units, activation, dropout):\n",
    "\n",
    "    # forward pass the time series through the encoder network\n",
    "    for i in range(len(units)):\n",
    "        h = tf.keras.layers.Dense(units=units[i], activation=activation)(x if i == 0 else h)\n",
    "        h = tf.keras.layers.Dropout(rate=dropout)(h)\n",
    "\n",
    "    # derive the mean of the latent vector\n",
    "    mu = tf.keras.layers.Dense(units=2)(h)\n",
    "\n",
    "    # derive the standard deviation of the latent vector\n",
    "    sigma = tf.exp(0.5 * tf.keras.layers.Dense(units=2)(h))\n",
    "\n",
    "    # generate the latent vector\n",
    "    z = mu + sigma * tf.random.normal(shape=(tf.shape(h)[0], 2))\n",
    "\n",
    "    return mu, sigma, z\n",
    "\n",
    "\n",
    "def decoder(z, features, units, activation, dropout):\n",
    "\n",
    "    # forward pass the latent vector through the decoder network (note that the decoder has\n",
    "    # the same layers as the encoder, but in reversed order)\n",
    "    for i in range(1, 1 + len(units)):\n",
    "        h = tf.keras.layers.Dense(units=units[-i], activation=activation)(z if i == 1 else h)\n",
    "        h = tf.keras.layers.Dropout(rate=dropout)(h)\n",
    "\n",
    "    # forward pass the output of the decoder network through a dense layer with number of\n",
    "    # units equal to the number of time series; this ensures that the reconstructions have\n",
    "    # the same shape as the inputs\n",
    "    xhat = tf.keras.layers.Dense(units=features)(h)\n",
    "\n",
    "    return xhat\n",
    "\n",
    "\n",
    "def discriminator(z, units, activation, dropout):\n",
    "\n",
    "    # forward pass the latent vector through the discriminator network\n",
    "    for i in range(len(units)):\n",
    "        h = tf.keras.layers.Dense(units=units[i], activation=activation)(z if i == 0 else h)\n",
    "        h = tf.keras.layers.Dropout(rate=dropout)(h)\n",
    "\n",
    "    # forward pass the output of the discriminator network through a dense layer with\n",
    "    # one unit and with sigmoid activation in order to get a vector of probabilities\n",
    "    p = tf.keras.layers.Dense(units=1, activation='sigmoid')(h)\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "class VAE():\n",
    "    def __init__(self, data, units, activation, dropout):\n",
    "\n",
    "        # difference the time series\n",
    "        x = data.diff().iloc[1:, :]\n",
    "\n",
    "\n",
    "        # scale the time series\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(x)\n",
    "        x = scaler.transform(x)\n",
    "\n",
    "        # extract the number of samples\n",
    "        samples = x.shape[0]\n",
    "\n",
    "        # extract the number of time series\n",
    "        features = x.shape[1]\n",
    "\n",
    "        # build the model\n",
    "        def build_model():\n",
    "\n",
    "            tf.random.set_seed(42)\n",
    "\n",
    "            # define the input\n",
    "            x = tf.keras.layers.Input(shape=(features,))\n",
    "\n",
    "            # forward pass the input through the encoder and get the latent representation\n",
    "            mu, sigma, z = encoder(x, units, activation, dropout)\n",
    "\n",
    "\n",
    "\n",
    "            # forward pass the latent representation through the decoder and get the reconstructions\n",
    "            xhat = decoder(z, features, units, activation, dropout)\n",
    "\n",
    "            # create the model\n",
    "            model = tf.keras.models.Model(x, [mu, sigma, z, xhat])\n",
    "            model.build(input_shape=(features,))\n",
    "\n",
    "            return model\n",
    "\n",
    "        # save the inputs\n",
    "        self.x = x\n",
    "        self.model = build_model()\n",
    "        self.scaler = scaler\n",
    "        self.samples = samples\n",
    "        self.features = features\n",
    "\n",
    "\n",
    "    def fit(self, batch_size, learning_rate, epochs, verbose):\n",
    "\n",
    "        tf.random.set_seed(42)\n",
    "\n",
    "        # instantiate the optimizer\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "        # define the training loop\n",
    "        @tf.function\n",
    "        def train_step(x):\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                # get the latent vector and its parameters from the encoder, and the reconstructions from the decoder\n",
    "                mu, sigma, z, xhat = self.model(x)\n",
    "\n",
    "                # calculate the encoder loss; this is the Kullback-Leibler distance between the true distribution of the latent\n",
    "                # vector (which is assumed to be the standard normal distribution) and the distribution predicted by the encoder\n",
    "                encoder_loss = - 0.5 * tf.reduce_mean(tf.reduce_sum(1 + tf.math.log(sigma ** 2) - mu ** 2 - sigma ** 2, axis=-1))\n",
    "\n",
    "                # calculate the decoder loss; this is the Mean Squared Error between the true values of the time series and the\n",
    "                # values predicted (or reconstructed) by the decoder\n",
    "                decoder_loss = tf.reduce_mean(tf.reduce_sum((tf.cast(x, tf.float32) - tf.cast(xhat, tf.float32)) ** 2, axis=-1))\n",
    "\n",
    "                # calculate the total loss\n",
    "                loss = encoder_loss + decoder_loss\n",
    "\n",
    "                # calculate the gradient\n",
    "                gradient = tape.gradient(loss, self.model.trainable_variables)\n",
    "\n",
    "                # update the weights\n",
    "                optimizer.apply_gradients(zip(gradient, self.model.trainable_variables))\n",
    "\n",
    "                return loss\n",
    "\n",
    "        # generate the training batches\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(self.x)\n",
    "        dataset = dataset.cache().shuffle(self.samples).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "        # train the model\n",
    "        history = []\n",
    "        for epoch in range(epochs):\n",
    "            for x in dataset:\n",
    "                loss = train_step(x)\n",
    "                history.append({'epoch': 1 + epoch, 'loss': loss})\n",
    "            if verbose:\n",
    "                print('Epoch: {:,.0f}, Loss: {:,.4f}.'.format(1 + epoch, loss))\n",
    "\n",
    "        return pd.DataFrame(history)\n",
    "\n",
    "    def predict(self, data):\n",
    "\n",
    "        tf.random.set_seed(42)\n",
    "\n",
    "        # difference the time series\n",
    "        x = data.diff().iloc[1:, :]\n",
    "\n",
    "        # scale the time series\n",
    "        x = self.scaler.transform(x)\n",
    "\n",
    "        # get the latent vectors\n",
    "        z = self.model(x)[-2].numpy()\n",
    "\n",
    "        # organize the latent vectors in a data frame\n",
    "        z = pd.concat([pd.DataFrame(np.zeros((1, 2))), pd.DataFrame(z)], axis=0, ignore_index=True)\n",
    "\n",
    "        # get the reconstructions\n",
    "        xhat = self.model(x)[-1].numpy()\n",
    "\n",
    "        # transform the reconstructions back to the original scale and organize them in a data frame\n",
    "        xhat = pd.concat([data.iloc[:1, :], pd.DataFrame(self.scaler.inverse_transform(xhat), columns=data.columns)], axis=0, ignore_index=True).cumsum(axis=0)\n",
    "\n",
    "        return z, xhat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HPO for VAE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "class VAEOptimizer:\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 n_trials):\n",
    "        self.data = data\n",
    "        self.n_trials = n_trials\n",
    "\n",
    "    def optimize(self):\n",
    "        def objective(trial):\n",
    "\n",
    "                # Sample a given hyperparameter combination\n",
    "#                dimensions = trial.suggest_int('dimensions', low=2, high=2)\n",
    "                units = trial.suggest_categorical('generator_units', [(4, 16, 32, 16, 4), (32, 16, 8, 4, 2, 4, 8, 16), (20, 10, 8, 6, 4, 2), (10, 9, 8, 7, 6, 8, 6, 5, 4, 3)])\n",
    "                activation = trial.suggest_categorical('activation', ['relu', 'tanh', 'sigmoid'])\n",
    "                dropout = trial.suggest_float('dropout', low=0, high=0.5, step=0.1)\n",
    "                batch_size = trial.suggest_categorical('batch_size', [4000, 6000, 7500])\n",
    "                learning_rate = trial.suggest_float('learning_rate', low=0.00005, high=0.001, step=0.00005)\n",
    "                epochs = trial.suggest_int('epochs', low=2000, high=10000, step=1000)\n",
    "\n",
    "                # Create the VAE model with the sampled hyperparameters\n",
    "                vae_model = VAE(\n",
    "                    data=self.data,\n",
    "                    units=units,\n",
    "                    activation=activation,\n",
    "                    dropout=dropout\n",
    "                )\n",
    "\n",
    "                # Train the model and obtain training loss history\n",
    "                training_history = vae_model.fit(\n",
    "                    batch_size=batch_size,\n",
    "                    learning_rate=learning_rate,\n",
    "                    epochs=epochs,\n",
    "                    verbose=False\n",
    "                )\n",
    "                final_loss = training_history['loss'].iloc[-1]\n",
    "\n",
    "                # Return the final loss for optimization\n",
    "                return final_loss\n",
    "\n",
    "        # Maximize the objective function, i.e. minimize the loss\n",
    "        study = optuna.create_study(direction='minimize', sampler=optuna.samplers.RandomSampler(seed=42))\n",
    "        study.optimize(func=objective, n_trials=self.n_trials, n_jobs=-1)\n",
    "\n",
    "        # Extract the best hyperparameters and the best loss\n",
    "        self.best_params = study.best_params\n",
    "        self.best_loss = study.best_value\n",
    "\n",
    "        # Print the best hyperparameters and the best loss\n",
    "        print(f'Best hyperparameters: {self.best_params}\\n')\n",
    "        print(f'Best loss: {self.best_loss}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = VAEOptimizer(\n",
    "    data = x_train,\n",
    "    n_trials = 10\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer.optimize()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# VAE Model Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = VAE(\n",
    "    data=x_train,\n",
    "    units=[4, 16, 32, 16, 4],\n",
    "    activation='tanh',\n",
    "    dropout=0.1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history_vae = model.fit(\n",
    "    learning_rate = 0.0009000000000000001,\n",
    "    batch_size=4000,\n",
    "    epochs=5000,\n",
    "    verbose=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot Learning History Loss\n",
    "rc('font', family='Times New Roman')\n",
    "plt.figure(figsize=(8, 4), dpi=300)\n",
    "plt.plot(history_regression.history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.savefig('Reg_output.jpg', format='jpeg', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Data Performance for VAE (Reconstruction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the latent representations and the reconstructions.\n",
    "L_train, xhat_train = model.predict(data = x_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Latent Representations\n",
    "rc('font', family='Times New Roman')\n",
    "plt.figure(figsize=(8, 4), dpi=300)\n",
    "for i in range(L_train.shape[1]):\n",
    "    plt.plot(L_train.iloc[:, i], label=f'L_train{1 + i}')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Latent Vectors')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('VAE_LV_train_output.jpg', format='jpeg', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reconstructions\n",
    "rc('font', family='Times New Roman')\n",
    "num_columns = 2\n",
    "num_rows = len(x.columns) // num_columns + (len(x.columns) % num_columns > 0)\n",
    "fig, axes = plt.subplots(num_rows, num_columns, figsize=(8, 2 * num_rows), dpi=300)\n",
    "for i, s in enumerate(x.columns):\n",
    "    row = i // num_columns\n",
    "    col = i % num_columns\n",
    "    ax = axes[row, col] if num_rows > 1 else axes[col]\n",
    "    ax.plot(x_train[s], label='actual')\n",
    "    ax.plot(xhat_train[s], label='reconstructed')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title(f'P {s+1}_train')\n",
    "    ax.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.savefig('VAE_x_train_output.jpg', format='jpeg', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing Data Performance for VAE (Reconstruction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "L_test, xhat_test = model.predict(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Latent Representations\n",
    "rc('font', family='Times New Roman')\n",
    "plt.figure(figsize=(8, 4), dpi=300)\n",
    "for i in range(L_test.shape[1]):\n",
    "    plt.plot(L_test.iloc[:, i], label=f'L_test{1 + i}')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Latent Vectors')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('VAE_LV_test_output.jpg', format='jpeg', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reconstructions\n",
    "rc('font', family='Times New Roman')\n",
    "num_columns = 2\n",
    "num_rows = len(x.columns) // num_columns + (len(x.columns) % num_columns > 0)  # 向上取整\n",
    "fig, axes = plt.subplots(num_rows, num_columns, figsize=(8, 2 * num_rows), dpi=300)\n",
    "for i, s in enumerate(x.columns):\n",
    "    row = i // num_columns\n",
    "    col = i % num_columns\n",
    "    ax = axes[row, col] if num_rows > 1 else axes[col]  # 获取当前子图对象\n",
    "    ax.plot(x_test[s], label='actual')\n",
    "    ax.plot(xhat_test[s], label='reconstructed')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title(f'P {s+1}_test')\n",
    "    ax.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.savefig('VAE_x_test_output.jpg', format='jpeg', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Linear Regression (Training)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataReg = pd.concat([L_train, u_train, P5_train], axis=1)\n",
    "dataReg = dataReg.iloc[1:]\n",
    "dataReg = dataReg.reset_index(drop=True)\n",
    "dataReg.columns = ['L1_train', 'L2_train', 'u1_train', 'u2_train', 'P5_train']\n",
    "X_train = dataReg[['L1_train', 'L2_train', 'u1_train', 'u2_train']]\n",
    "y_train = dataReg['P5_train']  # (Use P5 in next row)\n",
    "X_train = X_train.iloc[:-1]\n",
    "y_train = y_train.iloc[:-1]\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "\n",
    "# linear activation\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "# Build Regression NN\n",
    "regression_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, input_dim=4, activation=linear),  # input layer\n",
    "    tf.keras.layers.Dense(1, activation=linear)  # output layer\n",
    "])\n",
    "\n",
    "# build model\n",
    "regression_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# train model\n",
    "history_regression = regression_model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot Learning History Loss\n",
    "rc('font', family='Times New Roman')\n",
    "plt.figure(figsize=(8, 4), dpi=300)\n",
    "plt.plot(history_regression.history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.savefig('VAE_Reg_output.jpg', format='jpeg', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Data Performance for Reg (Prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = regression_model.predict(X_train)\n",
    "\n",
    "rc('font', family='Times New Roman')\n",
    "plt.figure(figsize=(8, 4), dpi=300)\n",
    "plt.plot(y_train, label='True Values')\n",
    "plt.plot(np.concatenate([[np.nan], y_pred.flatten()]), label='Predicted Values')\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('P5')\n",
    "plt.legend()\n",
    "plt.savefig('VAE_P5_pred_train_output.jpg', format='jpeg', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing Data Performance for Reg (Prediction)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = regression_model.predict(X_test)\n",
    "\n",
    "rc('font', family='Times New Roman')\n",
    "plt.figure(figsize=(8, 4), dpi=300)\n",
    "plt.plot(y_test, label='True Values')\n",
    "plt.plot(np.concatenate([[np.nan], y_pred.flatten()]), label='Predicted Values')\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('P5')\n",
    "plt.legend()\n",
    "plt.savefig('VAE_P5_pred_test_output.jpg', format='jpeg', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# VAER Model Build"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def encoder(x, units, activation, dropout):\n",
    "\n",
    "    # forward pass the time series through the encoder network\n",
    "    for i in range(len(units)):\n",
    "        h = tf.keras.layers.Dense(units=units[i], activation=activation)(x if i == 0 else h)\n",
    "        h = tf.keras.layers.Dropout(rate=dropout)(h)\n",
    "\n",
    "    # derive the mean of the latent vector\n",
    "    mu = tf.keras.layers.Dense(units=2)(h)\n",
    "\n",
    "    # derive the standard deviation of the latent vector\n",
    "    sigma = tf.exp(0.5 * tf.keras.layers.Dense(units=2)(h))\n",
    "\n",
    "    # generate the latent vector\n",
    "    z = mu + sigma * tf.random.normal(shape=(tf.shape(h)[0], 2))\n",
    "\n",
    "    return mu, sigma, z\n",
    "\n",
    "\n",
    "def decoder(z, features, units, activation, dropout):\n",
    "\n",
    "    # forward pass the latent vector through the decoder network (note that the decoder has\n",
    "    # the same layers as the encoder, but in reversed order)\n",
    "    for i in range(1, 1 + len(units)):\n",
    "        h = tf.keras.layers.Dense(units=units[-i], activation=activation)(z if i == 1 else h)\n",
    "        h = tf.keras.layers.Dropout(rate=dropout)(h)\n",
    "\n",
    "    # forward pass the output of the decoder network through a dense layer with number of\n",
    "    # units equal to the number of time series; this ensures that the reconstructions have\n",
    "    # the same shape as the inputs\n",
    "    xhat = tf.keras.layers.Dense(units=features)(h)\n",
    "\n",
    "    return xhat\n",
    "\n",
    "\n",
    "def discriminator(z, units, activation, dropout):\n",
    "\n",
    "    # forward pass the latent vector through the discriminator network\n",
    "    for i in range(len(units)):\n",
    "        h = tf.keras.layers.Dense(units=units[i], activation=activation)(z if i == 0 else h)\n",
    "        h = tf.keras.layers.Dropout(rate=dropout)(h)\n",
    "\n",
    "    # forward pass the output of the discriminator network through a dense layer with\n",
    "    # one unit and with sigmoid activation in order to get a vector of probabilities\n",
    "    p = tf.keras.layers.Dense(units=1, activation='sigmoid')(h)\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "class VAE():\n",
    "    def __init__(self, data, units, activation, dropout, u, p5):\n",
    "\n",
    "        # difference the time series\n",
    "        x = data.diff().iloc[1:, :]\n",
    "        u = u.diff().iloc[1:, :]\n",
    "        p5 = p5.diff().iloc[1:]\n",
    "\n",
    "        # scale the time series\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(x)\n",
    "        x = scaler.transform(x)\n",
    "\n",
    "        scaler2 = StandardScaler()\n",
    "        scaler2.fit(u)\n",
    "        u = scaler2.transform(u)\n",
    "\n",
    "        p5_2d = p5.values.reshape(-1, 1)\n",
    "        scaler3 = StandardScaler()\n",
    "        scaler3.fit(p5_2d)\n",
    "        p5 = scaler3.transform(p5_2d)\n",
    "\n",
    "        # extract the number of samples\n",
    "        samples = x.shape[0]\n",
    "\n",
    "        # extract the number of time series\n",
    "        features = x.shape[1]\n",
    "\n",
    "        # build the model\n",
    "        def build_model():\n",
    "\n",
    "            tf.random.set_seed(42)\n",
    "\n",
    "            # define the input\n",
    "            x = tf.keras.layers.Input(shape=(features,))\n",
    "\n",
    "            # forward pass the input through the encoder and get the latent representation\n",
    "            mu, sigma, z = encoder(x, units, activation, dropout)\n",
    "\n",
    "\n",
    "\n",
    "            # forward pass the latent representation through the decoder and get the reconstructions\n",
    "            xhat = decoder(z, features, units, activation, dropout)\n",
    "\n",
    "            # create the model\n",
    "            model = tf.keras.models.Model(x, [mu, sigma, z, xhat])\n",
    "            model.build(input_shape=(features,))\n",
    "\n",
    "            return model\n",
    "\n",
    "        # save the inputs\n",
    "        self.x = x\n",
    "        self.model = build_model()\n",
    "        self.scaler = scaler\n",
    "        self.scaler2 = scaler2\n",
    "        self.scaler3 = scaler3\n",
    "        self.samples = samples\n",
    "        self.features = features\n",
    "        self.u = u\n",
    "        self.p5 = p5\n",
    "\n",
    "\n",
    "    def fit(self, batch_size, learning_rate, epochs, verbose):\n",
    "\n",
    "        tf.random.set_seed(42)\n",
    "\n",
    "        # instantiate the optimizer\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "        # define the training loop\n",
    "        @tf.function\n",
    "        def train_step(x):\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                # get the latent vector and its parameters from the encoder, and the reconstructions from the decoder\n",
    "                mu, sigma, z, xhat = self.model(x)\n",
    "\n",
    "                # calculate the encoder loss; this is the Kullback-Leibler distance between the true distribution of the latent\n",
    "                # vector (which is assumed to be the standard normal distribution) and the distribution predicted by the encoder\n",
    "                encoder_loss = - 0.5 * tf.reduce_mean(tf.reduce_sum(1 + tf.math.log(sigma ** 2) - mu ** 2 - sigma ** 2, axis=-1))\n",
    "\n",
    "                # calculate the decoder loss; this is the Mean Squared Error between the true values of the time series and the\n",
    "                # values predicted (or reconstructed) by the decoder\n",
    "                decoder_loss = tf.reduce_mean(tf.reduce_sum((tf.cast(x, tf.float32) - tf.cast(xhat, tf.float32)) ** 2, axis=-1))\n",
    "\n",
    "                # calculate the total loss\n",
    "                loss = encoder_loss + decoder_loss\n",
    "\n",
    "                # calculate the gradient\n",
    "                gradient = tape.gradient(loss, self.model.trainable_variables)\n",
    "\n",
    "                # update the weights\n",
    "                optimizer.apply_gradients(zip(gradient, self.model.trainable_variables))\n",
    "\n",
    "                return loss\n",
    "\n",
    "        # generate the training batches\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(self.x)\n",
    "        dataset = dataset.cache().shuffle(self.samples).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "        # train the model\n",
    "        history = []\n",
    "        for epoch in range(epochs):\n",
    "            for x in dataset:\n",
    "                loss = train_step(x)\n",
    "                history.append({'epoch': 1 + epoch, 'loss': loss})\n",
    "            if verbose:\n",
    "                print('Epoch: {:,.0f}, Loss: {:,.4f}.'.format(1 + epoch, loss))\n",
    "\n",
    "        return pd.DataFrame(history)\n",
    "\n",
    "    def predict(self, data):\n",
    "\n",
    "        tf.random.set_seed(42)\n",
    "\n",
    "        # difference the time series\n",
    "        x = data.diff().iloc[1:, :]\n",
    "\n",
    "        # scale the time series\n",
    "        x = self.scaler.transform(x)\n",
    "\n",
    "        # get the latent vectors\n",
    "        z = self.model(x)[-2].numpy()\n",
    "\n",
    "        # organize the latent vectors in a data frame\n",
    "        z = pd.concat([pd.DataFrame(np.zeros((1, 2))), pd.DataFrame(z)], axis=0, ignore_index=True)\n",
    "\n",
    "        # get the reconstructions\n",
    "        xhat = self.model(x)[-1].numpy()\n",
    "\n",
    "        # transform the reconstructions back to the original scale and organize them in a data frame\n",
    "        xhat = pd.concat([data.iloc[:1, :], pd.DataFrame(self.scaler.inverse_transform(xhat), columns=data.columns)], axis=0, ignore_index=True).cumsum(axis=0)\n",
    "\n",
    "        return z, xhat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class VAEWithRegression(VAE):\n",
    "    def __init__(self, data, units, activation, dropout, u, p5, regression_hidden_units):\n",
    "        super().__init__(data, units, activation, dropout, u, p5)\n",
    "        self.regression_model = None\n",
    "        self.regression_hidden_units = regression_hidden_units\n",
    "        self.build_regression_model()\n",
    "\n",
    "    def build_regression_model(self):\n",
    "            z_input = tf.keras.layers.Input(shape=(2,))\n",
    "            u_input = tf.keras.layers.Input(shape=(2,))\n",
    "            regression_input = tf.keras.layers.Concatenate()([z_input, u_input])\n",
    "\n",
    "            h = regression_input\n",
    "            for units in self.regression_hidden_units:\n",
    "                h = tf.keras.layers.Dense(units, activation='linear')(h)\n",
    "            p5_pred = tf.keras.layers.Dense(1, activation='linear')(h)\n",
    "\n",
    "\n",
    "            self.regression_model = tf.keras.models.Model([z_input, u_input], p5_pred)\n",
    "\n",
    "\n",
    "    def fit_combined(self, batch_size, learning_rate, epochs, verbose):\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "        @tf.function\n",
    "        def train_step_combined(x, u, p5):\n",
    "            with tf.GradientTape() as tape:\n",
    "                mu, sigma, z, xhat = self.model(x)\n",
    "                p5_pred = self.regression_model([z, u])\n",
    "\n",
    "                encoder_loss = - 0.5 * tf.reduce_mean(tf.reduce_sum(1 + tf.math.log(sigma ** 2) - mu ** 2 - sigma ** 2, axis=-1))\n",
    "                decoder_loss = tf.reduce_mean(tf.reduce_sum((tf.cast(x, tf.float32) - tf.cast(xhat, tf.float32)) ** 2, axis=-1))\n",
    "                vae_loss = encoder_loss + decoder_loss\n",
    "                regression_loss = tf.reduce_mean(tf.reduce_sum(tf.keras.losses.mean_squared_error(p5, p5_pred), axis=-1))\n",
    "                total_loss = vae_loss + regression_loss\n",
    "\n",
    "                gradients = tape.gradient(total_loss, self.model.trainable_variables + self.regression_model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, self.model.trainable_variables + self.regression_model.trainable_variables))\n",
    "\n",
    "                return total_loss\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((self.x, self.u, self.p5))\n",
    "        dataset = dataset.cache().shuffle(self.samples).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "        history = []\n",
    "        for epoch in range(epochs):\n",
    "            for x, u, p5 in dataset:\n",
    "                loss = train_step_combined(x, u, p5)\n",
    "                history.append({'epoch': epoch + 1, 'loss': loss.numpy()})\n",
    "            if verbose:\n",
    "                print('Epoch: {:,.0f}, Loss: {:,.4f}.'.format(epoch + 1, loss))\n",
    "\n",
    "        return pd.DataFrame(history)\n",
    "\n",
    "    def predict_combined(self, data, u_data):\n",
    "        z, xhat = self.predict(data)\n",
    "        p5_pred = self.regression_model([z.values, u_data.values]).numpy()\n",
    "        p5_pred = pd.concat([pd.DataFrame(np.zeros((1, 1))), pd.DataFrame(self.scaler3.inverse_transform(p5_pred))], axis=0, ignore_index=True)\n",
    "\n",
    "        return z, xhat, p5_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HPO for VAER"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "class VAEROptimizer:\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 u,\n",
    "                 p5,\n",
    "                 regression_hidden_units,\n",
    "                 n_trials):\n",
    "        self.data = data\n",
    "        self.u = u\n",
    "        self.p5 = p5\n",
    "        self.n_trials = n_trials\n",
    "        self.regression_hidden_units = regression_hidden_units\n",
    "\n",
    "    def optimize(self):\n",
    "        def objective(trial):\n",
    "\n",
    "                # Sample a given hyperparameter combination\n",
    "#                dimensions = trial.suggest_int('dimensions', low=2, high=2)\n",
    "                units = trial.suggest_categorical('generator_units', [(4, 16, 32, 16, 4), (32, 16, 8, 4, 2, 4, 8, 16), (20, 10, 8, 6, 4, 2), (10, 9, 8, 7, 6, 8, 6, 5, 4, 3)])\n",
    "                activation = trial.suggest_categorical('activation', ['relu', 'tanh', 'sigmoid'])\n",
    "                dropout = trial.suggest_float('dropout', low=0, high=0.5, step=0.1)\n",
    "                batch_size = trial.suggest_categorical('batch_size', [5500, 6500, 7500])\n",
    "                learning_rate = trial.suggest_float('learning_rate', low=0.00005, high=0.001, step=0.00005)\n",
    "                epochs = trial.suggest_int('epochs', low=2000, high=10000, step=1000)\n",
    "\n",
    "                # Create the VAER model with the sampled hyperparameters\n",
    "                vaer_model = VAEWithRegression(\n",
    "                    data=self.data,\n",
    "                    units=units,\n",
    "                    activation=activation,\n",
    "                    dropout=dropout,\n",
    "                    u=self.u,\n",
    "                    p5=self.p5,\n",
    "                    regression_hidden_units = self.regression_hidden_units # Adjust as needed\n",
    "                )\n",
    "\n",
    "                # Train the model and obtain training loss history\n",
    "                training_history = vaer_model.fit_combined(\n",
    "                    batch_size=batch_size,\n",
    "                    learning_rate=learning_rate,\n",
    "                    epochs=epochs,\n",
    "                    verbose=False\n",
    "                )\n",
    "                final_loss = training_history['loss'].iloc[-1]\n",
    "\n",
    "                # Return the final loss for optimization\n",
    "                return final_loss\n",
    "\n",
    "        # Maximize the objective function, i.e. minimize the loss\n",
    "        study = optuna.create_study(direction='minimize', sampler=optuna.samplers.RandomSampler(seed=42))\n",
    "        study.optimize(func=objective, n_trials=self.n_trials, n_jobs=-1)\n",
    "\n",
    "        # Extract the best hyperparameters and the best loss\n",
    "        self.best_params = study.best_params\n",
    "        self.best_loss = study.best_value\n",
    "\n",
    "        # Print the best hyperparameters and the best loss\n",
    "        print(f'Best hyperparameters: {self.best_params}\\n')\n",
    "        print(f'Best loss: {self.best_loss}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = VAEROptimizer(\n",
    "    data = x_train,\n",
    "    u=u_train,\n",
    "    p5=P5_train,\n",
    "    regression_hidden_units=[10, 5],\n",
    "    n_trials = 10\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer.optimize()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# VAER Model Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build the model\n",
    "combined_model = VAEWithRegression(\n",
    "    data=x_train,\n",
    "    units=[4, 16, 32, 16, 4],\n",
    "    activation='tanh',\n",
    "    dropout=0.4,\n",
    "    u=u_train,\n",
    "    p5=P5_train,\n",
    "    regression_hidden_units=[10, 5]  # Adjust as needed\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training\n",
    "history = combined_model.fit_combined(\n",
    "    batch_size=7500,\n",
    "    learning_rate=0.001,\n",
    "    epochs=8000,\n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot Learning History Loss\n",
    "rc('font', family='Times New Roman')\n",
    "plt.figure(figsize=(8, 4), dpi=300)\n",
    "plt.plot(history['epoch'], history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Total Loss')\n",
    "plt.savefig('VAER_loss_output.jpg', format='jpeg', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Data Performance for VAER (Reconstruction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "L_train, xhat_train, p5_pred_train = combined_model.predict_combined(data = x_train, u_data = u_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Latent Representations\n",
    "rc('font', family='Times New Roman')\n",
    "plt.figure(figsize=(8, 4), dpi=300)\n",
    "for i in range(L_train.shape[1]):\n",
    "    plt.plot(L_train.iloc[:, i], label=f'L_train{1 + i}')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Latent Vectors')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('VAER_LV_train_output.jpg', format='jpeg', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reconstructions\n",
    "rc('font', family='Times New Roman')\n",
    "num_columns = 2\n",
    "num_rows = len(x.columns) // num_columns + (len(x.columns) % num_columns > 0)\n",
    "fig, axes = plt.subplots(num_rows, num_columns, figsize=(8, 2 * num_rows), dpi=300)\n",
    "for i, s in enumerate(x.columns):\n",
    "    row = i // num_columns\n",
    "    col = i % num_columns\n",
    "    ax = axes[row, col] if num_rows > 1 else axes[col]\n",
    "    ax.plot(x_train[s], label='actual')\n",
    "    ax.plot(xhat_train[s], label='reconstructed')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title(f'P {s+1}_train')\n",
    "    ax.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.savefig('VAER_x_train_output.jpg', format='jpeg', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing Data Performance for VAER (Reconstruction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "L_test, xhat_test, p5_pred_test = combined_model.predict_combined(x_test, u_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rc('font', family='Times New Roman')\n",
    "plt.figure(figsize=(8, 4), dpi=300)\n",
    "for i in range(L_test.shape[1]):\n",
    "    plt.plot(L_test.iloc[:, i], label=f'L_test{1 + i}')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Latent Vectors')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('VAER_LV_test_output.jpg', format='jpeg', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rc('font', family='Times New Roman')\n",
    "num_columns = 2\n",
    "num_rows = len(x.columns) // num_columns + (len(x.columns) % num_columns > 0)  # 向上取整\n",
    "fig, axes = plt.subplots(num_rows, num_columns, figsize=(8, 2 * num_rows), dpi=300)\n",
    "for i, s in enumerate(x.columns):\n",
    "    row = i // num_columns\n",
    "    col = i % num_columns\n",
    "    ax = axes[row, col] if num_rows > 1 else axes[col]  # 获取当前子图对象\n",
    "\n",
    "    ax.plot(x_test[s], label='actual')\n",
    "    ax.plot(xhat_test[s], label='reconstructed')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title(f'P {s+1}_test')\n",
    "    ax.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.savefig('VAER_x_test_output.jpg', format='jpeg', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Data Performance for VAER (Prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rc('font', family='Times New Roman')\n",
    "plt.figure(figsize=(8, 4), dpi=300)\n",
    "plt.plot(P5_train, label='True Values')\n",
    "plt.plot(p5_pred_train, label='Predicted Values')\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('P5')\n",
    "plt.legend()\n",
    "plt.savefig('VAER_P5_pred_train_output.jpg', format='jpeg', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing Data Performance for VAER (Prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.show()\n",
    "rc('font', family='Times New Roman')\n",
    "plt.figure(figsize=(8, 4), dpi=300)\n",
    "plt.plot(P5_test, label='True Values')\n",
    "plt.plot(p5_pred_test, label='Predicted Values')\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('P5')\n",
    "plt.legend()\n",
    "plt.savefig('VAER_P5_pred_test_output.jpg', format='jpeg', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}